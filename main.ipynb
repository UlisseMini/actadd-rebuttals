{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for first run on vast.ai\n",
    "%pip install transformer_lens matplotlib openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, Union, List\n",
    "from openai import OpenAI, RateLimitError\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import random\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI client & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_yes_no(prompt: str) -> bool:\n",
    "    done = False\n",
    "    backoff = 4\n",
    "    while not done:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful assistant that answers yes or no questions. Respond with 'yes' or 'no' only.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=3,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return \"yes\" in response.choices[0].message.content.lower()\n",
    "        except RateLimitError as e:\n",
    "            print(f\"Rate limit error: {e}\")\n",
    "            time.sleep(backoff)\n",
    "            backoff *= 2\n",
    "            if backoff > 128:\n",
    "                raise e\n",
    "\n",
    "def batch_yes_no(batch: List[str], question_fn, progress=True, num_workers=20) -> List[bool]:\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    \n",
    "    # Process in parallel batches of num_workers\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(question_fn, text) for text in batch]\n",
    "        results = list(tqdm(\n",
    "            (f.result() for f in futures),\n",
    "            total=len(batch),\n",
    "            desc=\"Evaluating batch\", \n",
    "            disable=not progress\n",
    "        ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def related(big_batch: List[str], topic: str, progress=True, num_workers=20) -> List[bool]:\n",
    "    question_fn = lambda text: ask_yes_no(f\"Is this text related to {topic}? Text:\\n\\n{text}\")\n",
    "    return batch_yes_no(big_batch, question_fn, progress, num_workers)\n",
    "\n",
    "def coherent(batch: List[str], progress=True, num_workers=20) -> List[bool]:\n",
    "    question_fn = lambda text: ask_yes_no(f\"Is this text extremely incoherent? Text:\\n\\n{text}\")\n",
    "    return batch_yes_no(batch, question_fn, progress, num_workers)\n",
    "\n",
    "related([\"I like the mona lisa\", \"Walruses are fascinating creatures\"], \"art\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"I got stuck in traffic (I live in Sicily) on the way to the theater (at a military base) to see Superman Returns, was \",\n",
    "          \"I don't know if this type of movie was as clich\\u00e9 then as it seems to be now.<br /><br />Considering how many \\\"Bad News\",\n",
    "          \"Forest of the Damned starts out as five young friends, brother & sister Emilio (Richard Cambridge) & Ally (Sophie Holland) along with J\",\n",
    "          \"I saw this DVD on sale and bought it without a second thought, despite not even having known it was out since this is one of my favorite books of\",\n",
    "          \"I don't know much about Tobe Hooper, or why he gets his name in the title, but maybe he shouldn't have bothered. As another\",\n",
    "          \"I liked the understated character that Laura Linney played in 'Love Actually', and she is very good in 'Man of the Year'.<br /><\",\n",
    "          \"This film is probably the worst movie I have watched in a very long time. The acting is so wooden a door could have done a better job. The\",\n",
    "          \"This is the kind of film one watches in gape-jawed, horrified silence, and yet continues to watch, mesmerized, as if watching a\",\n",
    "          \"Sorry I couldn't disagree more,with the last comments. frankly I thought this was worse than Carry on Columbus, enough said. Last film for THE usually\",\n",
    "          \"Next to the slasher films of the 1970s and 80s, ones about the walking dead were probably the second most popular horror sub-gen\", \"This is the first Guinea Pig film from Japan and this is the sickest, in my opinion. A bunch of guys torture a girl for several days before\", \"\\\"True\\\" story of a late monster that appears when an American industrial plant begins polluting the waters. Amusing, though not really good, monster film\", \"Even if you could get past the idea that these boring characters personally witnessed every Significant Moment of the 1960s (ok, so Katie didn't join\", \"I saw the MST3K version of \\\"Deathstalker III\\\" and loved the movie so much -- even \\\"unmystied\\\" -- that I\", \"I have a question for the writers and producers of \\\"Prozac Nation\\\": What is the root cause and what is the solution to the widespread problem of personal\"]\n",
    "\n",
    "results = coherent(inputs)\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"{inputs[i]}\\n{r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)  # save memory\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_name = \"gpt2-xl\"\n",
    "device = \"cuda:0\"\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "  model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen = lambda prompt: model.to_tokens(prompt).shape[1]\n",
    "pad_right = lambda prompt, length: prompt + \" \" * (length - tlen(prompt))\n",
    "\n",
    "def pad_both(p_add, p_sub):\n",
    "  l = max(tlen(p_add), tlen(p_sub))\n",
    "  return pad_right(p_add, l), pad_right(p_sub, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resid_pre(prompt: str, layer: int):\n",
    "    name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    cache, caching_hooks, _ = model.get_caching_hooks(lambda n: n == name)\n",
    "    with model.hooks(fwd_hooks=caching_hooks):\n",
    "        _ = model(prompt)\n",
    "    return cache[name]\n",
    "\n",
    "\n",
    "def get_act_diff(prompt_add: str, prompt_sub: str, layer: int):\n",
    "    act_add = get_resid_pre(prompt_add, layer)\n",
    "    act_sub = get_resid_pre(prompt_sub, layer)\n",
    "    return act_add - act_sub # if this errors you forgot to pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def ave_hook(act_diff, resid_pre, hook):\n",
    "    global h\n",
    "    h = hook\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return  # caching in model.generate for new tokens\n",
    "\n",
    "    # We only add to the prompt (first call), not the generated tokens.\n",
    "    ppos, apos = resid_pre.shape[1], act_diff.shape[1]\n",
    "    assert apos <= ppos, f\"More mod tokens ({apos}) then prompt tokens ({ppos})!\"\n",
    "\n",
    "    # add to the beginning (position-wise) of the activations\n",
    "    resid_pre[:, :apos, :] += act_diff\n",
    "\n",
    "\n",
    "from typing import Union, List\n",
    "def hooked_generate(prompt_batch: Union[List[str], torch.Tensor], fwd_hooks=[], seed=None, verbose=False, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        if isinstance(prompt_batch, list):\n",
    "            tokenized = model.to_tokens(prompt_batch)\n",
    "        else:\n",
    "            tokenized = prompt_batch\n",
    "        r = model.generate(input=tokenized, do_sample=True, verbose=verbose, **kwargs)\n",
    "    return r\n",
    "\n",
    "\n",
    "def generate_hooked(prompt_batch: List[str], prompt_add: str, prompt_sub: str, act_name: int, coeff: float, verbose=False, max_new_tokens=50):\n",
    "  prompt_add, prompt_sub = pad_both(prompt_add, prompt_sub)\n",
    "  act_diff = coeff*get_act_diff(prompt_add, prompt_sub, act_name)\n",
    "  editing_hooks = [(f\"blocks.{act_name}.hook_resid_pre\", partial(ave_hook, act_diff))]\n",
    "  hooked_res = hooked_generate(prompt_batch, editing_hooks, seed=SEED, verbose=verbose, **sampling_kwargs, max_new_tokens=max_new_tokens)\n",
    "  return hooked_res\n",
    "\n",
    "\n",
    "def generate_both(prompt_batch: List[str], prompt_add: str, prompt_sub: str, act_name: int, coeff: float):\n",
    "  hooked_res = generate_hooked(prompt_batch, prompt_add, prompt_sub, act_name, coeff)\n",
    "  vanilla_res = generate_hooked(prompt_batch, prompt_add, prompt_sub, act_name, 0.0)\n",
    "  return hooked_res, vanilla_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific to the love/hate example\n",
    "# commented out bc wastes precious memory demonstrating stuff that works\n",
    "if 1:\n",
    "    prompt_add, prompt_sub = \"science\", \"\"\n",
    "    coeff = 4.0\n",
    "    act_name = 6\n",
    "    prompt = \"Did you know that\"\n",
    "\n",
    "    print(\"Generating...\")\n",
    "    hooked_res, vanilla_res = generate_both([prompt]*20, prompt_add, prompt_sub, act_name, coeff)\n",
    "    print(\"Done generating!\")\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    # NOTE: Has endoftext a bunch too\n",
    "    hooked_strs = model.to_string(hooked_res[:, 1:])\n",
    "    vanilla_strs = model.to_string(vanilla_res[:, 1:])\n",
    "\n",
    "    hooked_rel = related(hooked_strs, prompt_add)\n",
    "    vanilla_rel = related(vanilla_strs, prompt_add)\n",
    "    print()\n",
    "    print(f\"Hooked: {np.mean(hooked_rel)}\")\n",
    "    print(f\"Vanilla: {np.mean(vanilla_rel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(SEED)\n",
    "tokenized = model.to_tokens([prompt]*20)\n",
    "unhooked_res = model.generate(tokenized, do_sample=True, max_new_tokens=50, **sampling_kwargs)\n",
    "unhooked_strs = model.to_string(unhooked_res[:, 1:])\n",
    "unhooked_rel = related(unhooked_strs, prompt_add)\n",
    "print(f\"Unhooked: {np.mean(unhooked_rel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "hf_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average': 2.634668080148117e-06,\n",
       " 'median': 4.75194498417153e-06,\n",
       " 'correct_perplexity': 2.8177933466544403e-06}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity code adapted from https://github.com/eth-sri/language-model-arithmetic/blob/6779b40702d5214525a21e51c91d5042f873bd81/src/model_arithmetic/evaluation.py#L285\n",
    "\n",
    "def my_perplexity(model, prompts: List[str], generated: List[str]):\n",
    "    prompts_tokenized = model.to_tokens(prompts)\n",
    "    generated_tokenized = model.to_tokens(generated)\n",
    "\n",
    "    output = model.forward(generated_tokenized, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "    # Create attention mask that excludes EOS tokens\n",
    "    attention_mask = (results != model.tokenizer.eos_token_id)\n",
    "\n",
    "    # output.shape is 1 less than results.shape, exclude last token (we cant compute loss for the last token)\n",
    "    attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "    # exclude prompt from perplexity calculation\n",
    "    output = output[:, prompts_tokenized.shape[1]:]\n",
    "    attention_mask = attention_mask[:, prompts_tokenized.shape[1]:]\n",
    "\n",
    "    # compute perplexities :)\n",
    "    perplexities = ((attention_mask * output).sum(-1) / attention_mask.sum(-1)).exp().tolist()\n",
    "    return perplexities\n",
    "\n",
    "\n",
    "\n",
    "# same function but do the forward pass for every input at once\n",
    "def compute_perplexities_batched(dataset, model, tokenizer, batch_size=8, **kwargs):\n",
    "    perplexities = []\n",
    "    sum_nllos = 0\n",
    "    n_tokens = 0\n",
    "    \n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Get input lengths for each sample in batch\n",
    "        input_lengths = []\n",
    "        combined_sentences = []\n",
    "        for _, sample in batch.iterrows():\n",
    "            if len(sample['generated']) == 0:\n",
    "                input_lengths.append(0)\n",
    "                continue\n",
    "            input_encodings = tokenizer(sample['input'], return_tensors='pt')\n",
    "            input_lengths.append(input_encodings['input_ids'].size(1))\n",
    "            combined_sentences.append(sample['input'] + sample['generated'])\n",
    "            \n",
    "        if not combined_sentences:\n",
    "            continue\n",
    "            \n",
    "        # Tokenize full batch\n",
    "        encodings = tokenizer(combined_sentences, return_tensors='pt', padding=True)\n",
    "        input_ids = encodings['input_ids'].to(model.device)\n",
    "        attention_mask = encodings['attention_mask'].to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # logits = model(input_ids, labels=input_ids, attention_mask=attention_mask).logits # hf_model\n",
    "            logits = model(input_ids, attention_mask=attention_mask) # transformerlens model\n",
    "            logprobs = logits.log_softmax(dim=-1)\n",
    "        \n",
    "        loss_func = torch.nn.NLLLoss(ignore_index=-100, reduction='sum')\n",
    "        \n",
    "        # Calculate loss for each sample in batch\n",
    "        for j in range(len(combined_sentences)):\n",
    "            input_length = input_lengths[j]\n",
    "            if input_length == 0:\n",
    "                perplexities.append(None)\n",
    "                continue\n",
    "                \n",
    "            loss = loss_func(\n",
    "                logprobs[j, input_length:-1, :].contiguous(),\n",
    "                input_ids[j, input_length+1:].contiguous()\n",
    "            )\n",
    "            loss = loss.to(torch.float32).detach().cpu().numpy()\n",
    "            \n",
    "            n_tokens_here = (attention_mask[j, input_length+1:] == 1).sum().item()\n",
    "            \n",
    "            if n_tokens_here > 0:\n",
    "                perplexity = np.exp(loss / n_tokens_here)\n",
    "                sum_nllos += loss\n",
    "                n_tokens += n_tokens_here\n",
    "                if not np.isnan(perplexity):\n",
    "                    perplexities.append(perplexity)\n",
    "                else:\n",
    "                    perplexities.append(None)\n",
    "            else:\n",
    "                perplexities.append(None)\n",
    "                \n",
    "    return perplexities, sum_nllos, n_tokens\n",
    "\n",
    "\n",
    "def get_perplexity(dataset, model, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of the generated sentences.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset to be used for evaluation. Has columns \"input\" (for input text), \"generated\" (for generated text). \n",
    "        model (PreTrainedModel): The model to be evaluated.\n",
    "        tokenizer (Tokenizer): The tokenizer to be used for tokenizing the sentences.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "    \"\"\"\n",
    "    perplexities, sum_nllos, n_tokens = compute_perplexities_batched(dataset, model, model.tokenizer, **kwargs)\n",
    "\n",
    "    average = np.mean(perplexities)\n",
    "    median = np.median(perplexities)\n",
    "    real = np.exp(sum_nllos / n_tokens)\n",
    "\n",
    "    return {\n",
    "        \"average\": average,\n",
    "        \"median\": median,\n",
    "        \"correct_perplexity\": real\n",
    "    }\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# NOTE: THIS IS WRONG (we passed full completions instead of just generations),\n",
    "# But that's STILL FINE because we're just making sure my batched version WORKS THE SAME.\n",
    "df = pd.DataFrame({\"input\": [prompt]*20, \"generated\": hooked_strs})\n",
    "hf_tokenizer.pad_token = model.tokenizer.pad_token\n",
    "model.device = next(model.parameters()).device\n",
    "r = get_perplexity(df, model)\n",
    "\n",
    "# gold standard (their code unmodified with hf model loaded separately)\n",
    "gold = {'average': 2.9299599115344255, 'median': 2.7279913006370267, 'correct_perplexity': 2.8266689020402422}\n",
    "\n",
    "# compute diff\n",
    "diff = {k: r[k] - gold[k] for k in gold}\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold standards (hf_model, before port to transformerlens)\n",
    "# 1. hooked_strs:\n",
    "{'average': 2.9299599115344255,\n",
    " 'median': 2.7279913006370267,\n",
    " 'correct_perplexity': 2.8266689020402422}\n",
    "\n",
    "# 2. vanilla_strs & unhooked_strs (same)\n",
    "{'average': 2.9692005455398056,\n",
    " 'median': 2.885386634383141,\n",
    " 'correct_perplexity': 2.8907230675863858}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'average': 3.307980924844742, 'median': 3.279552698135376}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities_per_sample = my_perplexity(model, hooked_res)\n",
    "{\n",
    "    'average': np.mean(perplexities_per_sample),\n",
    "    'median': np.median(perplexities_per_sample),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooked_res is coeff=4.0 hooked\n",
    "# vanilla_res is coeff=0 hooked\n",
    "# unhooked_res is pure model.generate no hooks\n",
    "for results in [hooked_res, vanilla_res, unhooked_res]:\n",
    "    output = model.forward(results, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "    # Create attention mask that excludes EOS tokens\n",
    "    attention_mask = (results != model.tokenizer.eos_token_id)\n",
    "\n",
    "    # output.shape is 1 less than results.shape, exclude last token (we cant compute loss for the last token)\n",
    "    attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "    # compute perplexities :)\n",
    "    perplexities = ((attention_mask * output).sum(-1) / attention_mask.sum(-1)).exp().tolist()\n",
    "    print(perplexities[:10])\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter imdb prompts to remove all things that already talk about one of the topics\n",
    "TOPICS = [\"art\", \"finance\", \"music\", \"politics\", \"science\", \"weddings\"]\n",
    "\n",
    "try:\n",
    "  #1/0 # uncomment to regenerate cache\n",
    "  imdb_prompts = json.load(open('imdb_prompts_filtered.json'))\n",
    "except Exception as e:\n",
    "  ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "  imdb_sample = random.sample(ds['test']['text'], 3000)\n",
    "  imdb_prompts = [x[:256] for x in imdb_sample]\n",
    "  imdb_prompts = [\" \".join(x.split(\" \")[:-1]) for x in imdb_prompts]\n",
    "  imdb_prompts[:10]\n",
    "\n",
    "  for topic in TOPICS:\n",
    "    rel = related(imdb_prompts, topic, num_workers=100, progress=True)\n",
    "    imdb_prompts = [x for x, y in zip(imdb_prompts, rel) if not y]\n",
    "    print(f\"{topic}: removed {sum(rel)} ({np.mean(rel)*100:.1f}%) new len prompts: {len(imdb_prompts)}\")\n",
    "    open('imdb_prompts_filtered.json', 'w').write(json.dumps(imdb_prompts))\n",
    "\n",
    "imdb_prompts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "COEFFICIENTS = [0.0, 0.5, 1.0, 2.0]\n",
    "SEED = 0\n",
    "# PROMPT_BATCH = [\"Did you know that\"] * 100 # old paper thing\n",
    "# char level instead of token level\n",
    "# PROMPT_BATCH = [\" \".join(x[:100].split(\" \")[:-1]) for x in imdb_prompts[:100]]\n",
    "\n",
    "# use model.to_tokens to truncate each to 32 tokens\n",
    "PROMPT_BATCH = [model.to_string(model.to_tokens(x)[0][:32].tolist()[1:]) for x in tqdm(imdb_prompts[:1000])]\n",
    "\n",
    "act_name = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_big_hooked_batch(prompt_batch: List[str], padd: str, psub: str, act_name: int, coeff: float):\n",
    "  \"Generate in cases where processing prompt_batch all at once would make us run out of memory.\"\n",
    "\n",
    "  batch_size = 60 # XXX: REDUCE THIS IF UR NOT ON A h100\n",
    "\n",
    "  res_strs = []\n",
    "  disable_progress = len(prompt_batch) < batch_size\n",
    "  for i in tqdm(range(0, len(prompt_batch), batch_size), desc=\"Prompt batch\", disable=disable_progress):\n",
    "    batch = prompt_batch[i:i+batch_size]\n",
    "    hooked_res = generate_hooked(batch, padd, psub, act_name, coeff)\n",
    "    res_strs += model.to_string(hooked_res[:, 1:])\n",
    "  return [x.replace(\"<|endoftext|>\", \"\") for x in res_strs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic steering\n",
    "\n",
    "Re-does the topic steering experiment. Very customizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "completions = {}\n",
    "results = {}\n",
    "\n",
    "PROMPT_TEMPLATE = \"I talk about {topic} constantly\"\n",
    "NEG_PROMPT_TEMPLATE = \"I do not talk about {topic} constantly\"\n",
    "\n",
    "# Initialize results\n",
    "for topic in TOPICS:\n",
    "    results[topic] = []\n",
    "\n",
    "threads = []\n",
    "\n",
    "# Generate completions and spawn related tasks\n",
    "for topic in tqdm(TOPICS, desc=\"topics\"):\n",
    "    prompt_add, prompt_sub = pad_both(PROMPT_TEMPLATE.format(topic=topic), \n",
    "                                    NEG_PROMPT_TEMPLATE.format(topic=topic))\n",
    "    completions[topic] = {}\n",
    "    \n",
    "    for coeff in COEFFICIENTS:\n",
    "        hooked_strs = generate_big_hooked_batch(PROMPT_BATCH, prompt_add, prompt_sub, \n",
    "                                              act_name, coeff)\n",
    "        completions[topic][coeff] = hooked_strs\n",
    "        \n",
    "        def run_related(t=topic, c=coeff, h=hooked_strs):\n",
    "            rel = related(h, t, num_workers=100, progress=False)\n",
    "            score = np.mean(rel)\n",
    "            results[t].append(score)\n",
    "            print(f\"topic {t} coeff {c} -- rel: {score}\")\n",
    "            \n",
    "        thread = Thread(target=run_related)\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "# Wait for all threads\n",
    "print(\"Waiting for threads to finish...\")\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full results\n",
    "\n",
    "import time\n",
    "\n",
    "fname = f\"results/{int(time.time())}.json\"\n",
    "with open(fname, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"results\": results,\n",
    "        \"completions\": completions,\n",
    "        \"meta\": {\n",
    "            \"coeffs\": COEFFICIENTS,\n",
    "            \"topics\": TOPICS,\n",
    "            \"act_name\": act_name,\n",
    "            \"seed\": SEED,\n",
    "            \"prompt_template\": PROMPT_TEMPLATE,\n",
    "            \"neg_prompt_template\": NEG_PROMPT_TEMPLATE,\n",
    "        },\n",
    "        \"prompts\": PROMPT_BATCH\n",
    "    }, f)\n",
    "\n",
    "print(f\"Saved results to {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results in case we want to plot a different one\n",
    "# TODO(easy): Past easy experiment graph explorer widget\n",
    "if 0:\n",
    "    fname = 'results/1732323963.json'\n",
    "    with open(fname) as f:\n",
    "        file_data = json.load(f)\n",
    "    results = file_data['results']\n",
    "    completions = file_data['completions']\n",
    "    COEFFICIENTS = file_data['meta']['coeffs']\n",
    "    TOPICS = file_data['meta']['topics']\n",
    "    act_name = file_data['meta']['act_name']\n",
    "    SEED = file_data['meta']['seed']\n",
    "    PROMPT_TEMPLATE = file_data['meta']['prompt_template']\n",
    "    NEG_PROMPT_TEMPLATE = file_data['meta']['neg_prompt_template']\n",
    "    PROMPT_BATCH = file_data['prompts']\n",
    "\n",
    "    print(\"Loaded results from\", fname, \"with\", len(PROMPT_BATCH), \"prompts\", \"and coeffs\", COEFFICIENTS, \"and prompt templates:\\n\", PROMPT_TEMPLATE, \"\\nand neg prompt templates:\\n\", NEG_PROMPT_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the topic steering plot WITH baseline included\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(TOPICS))\n",
    "width = 0.2\n",
    "\n",
    "subtract = True\n",
    "for i, coeff in enumerate(COEFFICIENTS):\n",
    "    relevance_scores = [results[topic][i]*100 for topic in TOPICS]\n",
    "    plt.bar(x + i * width, relevance_scores, width, label=f'c={coeff}')\n",
    "\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('% evaluated as relevant')\n",
    "plt.title('gpt-4o-mini scored relevance of ActAdd completions on generic topics')\n",
    "plt.xticks(x + width * (len(COEFFICIENTS) - 1) / 2, TOPICS)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the visualization WITHOUT baseline included (show diffs)\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(TOPICS))\n",
    "width = 0.2\n",
    "\n",
    "subtract = True\n",
    "for i, coeff in enumerate(COEFFICIENTS[1:]):\n",
    "    relevance_scores = [results[topic][i]*100 for topic in TOPICS]\n",
    "    plt.bar(x + i * width, relevance_scores, width, label=f'c={coeff}')\n",
    "\n",
    "plt.xlabel('Topic')\n",
    "# plt.ylabel('% evaluated as relevant minus no-editing baseline')\n",
    "plt.ylabel('Δ% Topic Relevance (vs Unedited)')\n",
    "plt.title('gpt-4o-mini scored relevance of ActAdd completions on generic topics')\n",
    "plt.xticks(x + width * (len(COEFFICIENTS[1:]) - 1) / 2, TOPICS)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works better for long 32 tok imdb prompts:\n",
    "# prompt_add, prompt_sub = pad_both(\"I talk about weddings constantly\", \"I do not talk about weddings constantly\")\n",
    "# coeff = 4.0\n",
    "\n",
    "# XXX: paper used https://arxiv.org/pdf/2308.10248v3\n",
    "# XXX: they also used ‘I went up to my friend and said’ as prompt, but\n",
    "# XXX: imdb prompts are long so this works worse unless we truncate\n",
    "# XXX: imdb prompts a bunch (what I've done here, to 16 chars.)\n",
    "prompt_add, prompt_sub = pad_both(\"weddings\", \"\")\n",
    "coeff = 1.0\n",
    "\n",
    "layer_completions = []\n",
    "# layers = [2, 6, 16, 35]\n",
    "layers = list(range(0, 48))\n",
    "pbatch = [p[:16] for p in PROMPT_BATCH] # XXX: Cheating? See comment above\n",
    "for layer in tqdm(layers + [-1]):\n",
    "    if layer != -1:\n",
    "      hooked_res = generate_hooked(pbatch, prompt_add, prompt_sub, layer, coeff) \n",
    "    else:\n",
    "      hooked_res = generate_hooked(pbatch, prompt_add, prompt_sub, 0, 0) # baseline\n",
    "\n",
    "    layer_completions.append([\n",
    "        x.replace(\"<|endoftext|>\", \"\") for x in model.to_string(hooked_res[:, 1:])\n",
    "    ])\n",
    "  \n",
    "baseline_completion = layer_completions.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "fname = f\"results/{int(time.time())}-weddings.json\"\n",
    "with open(fname, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"layer_completions\": layer_completions,\n",
    "        \"layers\": layers,\n",
    "        \"baseline_completion\": baseline_completion,\n",
    "        \"prompt_batch\": PROMPT_BATCH,\n",
    "        \"meta\": {\n",
    "            \"coeff\": coeff,\n",
    "            \"prompt_add\": prompt_add,\n",
    "            \"prompt_sub\": prompt_sub,\n",
    "            \"seed\": SEED,\n",
    "            \"act_name\": act_name,\n",
    "        }\n",
    "    }, f)\n",
    "    print(f\"wrote {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute wedding-related completions % (figure 3)\n",
    "wedding_words = (\n",
    "    \"wedding\", \"weddings\", \"wed\", \"marry\", \"married\",\n",
    "    \"marriage\", \"bride\", \"groom\", \"honeymoon\",\n",
    ")\n",
    "\n",
    "def contains_wedding_words(text: str) -> bool:\n",
    "    assert isinstance(text, str), f\"what the fuck is a '{type(text)}'\"\n",
    "    assert len(text) > 1, f\"bruh wtf just '{text}' is all i get\"\n",
    "    return any(word in wedding_words for word in text.lower().split())\n",
    "\n",
    "wedding_related_pct = [\n",
    "    np.mean([contains_wedding_words(x) for x in layer_completions])\n",
    "    for layer_completions in layer_completions\n",
    "]\n",
    "baseline_pct = np.mean([contains_wedding_words(x) for x in baseline_completion])\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(layers, wedding_related_pct, 'b-', linewidth=1)\n",
    "plt.axhline(y=baseline_pct, color='k', linestyle='--', alpha=0.3, label='baseline')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('% of completions containing wedding words')\n",
    "plt.grid(True, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph mean wedding word count per layer (figure 7)\n",
    "def wedding_word_count(text: str) -> int:\n",
    "    return sum(word in wedding_words for word in text.lower().split())\n",
    "\n",
    "\n",
    "wedding_word_counts = [\n",
    "    np.mean([wedding_word_count(x) for x in lc])\n",
    "    for lc in layer_completions\n",
    "]\n",
    "baseline_count = np.mean([wedding_word_count(x) for x in baseline_completion])\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(layers, wedding_word_counts, 'b-', linewidth=1)\n",
    "plt.axhline(y=baseline_count, color='k', linestyle='--', alpha=0.3, label='baseline')\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Mean wedding word count')\n",
    "plt.grid(True, alpha=0.2)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_BATCHES = {\n",
    "  \"positive\": pd.read_csv(\"prompts/sentiment_imdb_POSITIVE_gridsearch_subset (1).csv\")[\"input\"].tolist()[:60],\n",
    "  \"negative\": pd.read_csv(\"prompts/sentiment_imdb_NEGATIVE_gridsearch_subset (1).csv\")[\"input\"].tolist()[:60],\n",
    "  \"toxicity\": pd.read_csv(\"prompts/toxicity_pol_gridsearch_subset (1).csv\")[\"input\"].tolist()[:60],\n",
    "}\n",
    "SELECTION = \"toxicity\"\n",
    "\n",
    "PROMPT_BATCH = PROMPT_BATCHES[SELECTION]\n",
    "\n",
    "prompt_add, prompt_sub = {\n",
    "  \"negative\": (\"Love\", \"Hate\"),\n",
    "  \"positive\": (\"Hate\", \"Love\"), # padding reasons: \" \" is prob better\n",
    "  \"toxicity\": (\"Love\", \"Hate\"),\n",
    "}[SELECTION]\n",
    "\n",
    "max_new_tokens = 32 if SELECTION == \"toxicity\" else 64\n",
    "\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=1.0, freq_penalty=1.0) # for llama, new ones\n",
    "SEED = 0\n",
    "\n",
    "grid_to_sweep = list(itertools.product(range(0, 23, 3), [-3, -1, 0, 1, 3, 6, 12, 18]))\n",
    "# 32x32 overkill as fuck. should take around 3hrs for each dataset.\n",
    "# grid_to_sweep = list(itertools.product(\n",
    "#     range(0, 32),\n",
    "#     range(-9, 23),\n",
    "# ))\n",
    "\n",
    "exp_name = f\"{SELECTION}_n{len(grid_to_sweep)}\"\n",
    "exp_name += (\"_fp\" if \"freq_penalty\" in sampling_kwargs else \"\")\n",
    "exp_folder = f\"grid_results/{exp_name}\"\n",
    "os.makedirs(exp_folder, exist_ok=True)\n",
    "with open(f\"{exp_folder}/meta.json\", \"w\") as f:\n",
    "  json.dump({\n",
    "    \"prompt_add\": prompt_add,\n",
    "    \"prompt_sub\": prompt_sub,\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "    \"sampling_kwargs\": sampling_kwargs,\n",
    "    \"PROMPT_BATCH\": PROMPT_BATCH,\n",
    "  }, f)\n",
    "  print(f\"wrote {f.name}\")\n",
    "\n",
    "for i, (l, c) in tqdm(enumerate(grid_to_sweep), total=len(grid_to_sweep)):\n",
    "  results = generate_hooked(PROMPT_BATCH, prompt_add, prompt_sub, l, c, verbose=False, max_new_tokens=max_new_tokens)\n",
    "  results_str = model.to_string(results[:, 1:])\n",
    "\n",
    "  output = model.forward(results, return_type=\"loss\", loss_per_token=True)\n",
    "\n",
    "  # Create attention mask that excludes EOS tokens\n",
    "  attention_mask = (results != model.tokenizer.eos_token_id)\n",
    "\n",
    "  # output.shape is 1 less than results.shape, exclude last token (we cant compute loss for the last token)\n",
    "  attention_mask = attention_mask[:, :-1]\n",
    "\n",
    "  # compute perplexities :)\n",
    "  perplexities = ((attention_mask * output).sum(-1) / attention_mask.sum(-1)).exp().tolist()\n",
    "\n",
    "  # and json cuz idk why not\n",
    "  with open(f\"{exp_folder}/{i}_l{l}_c{c}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "      \"prompt_batch\": PROMPT_BATCH,\n",
    "      \"completions\": results_str,\n",
    "      \"perplexities\": perplexities,\n",
    "      \"layer\": l,\n",
    "      \"coeff\": c,\n",
    "      # TODO: More metrics (sentiment, toxicity, % coherent, ?)\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the json files and plot them\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_heatmap(exp_folder: str, SELECTION: str, values: str = \"perplexity\", annot: bool = False):\n",
    "    files = os.listdir(exp_folder)\n",
    "    data = []\n",
    "\n",
    "    for f in files:\n",
    "        if f == \"meta.json\":\n",
    "            continue\n",
    "\n",
    "        with open(exp_folder + \"/\" + f) as fp:\n",
    "            d = json.load(fp)\n",
    "            data.append({\n",
    "                \"layer\": d[\"layer\"],\n",
    "                \"coeff\": d[\"coeff\"], \n",
    "                \"perplexity\": np.mean(d[\"perplexities\"]), # avg over batch\n",
    "                \"sentiment\": np.mean(d[\"sentiment\"]),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    pivot = df.pivot(index=\"layer\", columns=\"coeff\", values=values)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.heatmap(pivot, cmap=\"viridis\", annot=annot, fmt='.1f')\n",
    "\n",
    "    dataset_name = f\"IMDB {SELECTION.title()} Reviews\" if SELECTION != \"toxicity\" else \"RealToxicityPrompts\"\n",
    "    plt.title(f\"{dataset_name}, Completion {values.title()} Heatmap\") \n",
    "\n",
    "    plt.ylabel(\"Layer\")\n",
    "    plt.xlabel(\"Coefficient\")\n",
    "    plt.savefig(f\"figures/{exp_name}_{values}_{'noannot_' if not annot else ''}heatmap.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "for annot in [False, True]:\n",
    "    plot_heatmap(exp_folder, SELECTION, \"perplexity\", annot)\n",
    "    plot_heatmap(exp_folder, SELECTION, \"sentiment\", annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all (for when plotting code updates)\n",
    "\n",
    "for exp_folder in os.listdir(\"grid_results\"):\n",
    "  selected_dataset = exp_folder.split(\"_\")[0]\n",
    "  plot_heatmap(f\"grid_results/{exp_folder}\", selected_dataset, \"perplexity\")\n",
    "  plot_heatmap(f\"grid_results/{exp_folder}\", selected_dataset, \"sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
