{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new metric over existing completions\n",
    "\n",
    "We don't want to recompute the expensive part of the grid, so we just\n",
    "add metrics on top of existing completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.998561680316925},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991401433944702}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis([\"I love you\", \"I hate you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding sentiment to negative_n1024_fp: 100%|██████████| 1025/1025 [00:00<00:00, 7802.46it/s]\n",
      "adding sentiment to positive_n64: 100%|██████████| 65/65 [00:00<00:00, 7865.83it/s]\n",
      "adding sentiment to toxicity_n1024: 100%|██████████| 1025/1025 [00:00<00:00, 3724.47it/s]\n",
      "adding sentiment to positive_n1024_fp: 100%|██████████| 1025/1025 [00:00<00:00, 7828.94it/s]\n",
      "adding sentiment to toxicity_n64: 100%|██████████| 65/65 [00:00<00:00, 3824.93it/s]\n",
      "adding sentiment to toxicity_n64_fp: 100%|██████████| 65/65 [00:00<00:00, 4043.75it/s]\n",
      "adding sentiment to positive_n64_fp: 100%|██████████| 4/4 [00:00<00:00, 5488.13it/s]\n",
      "adding sentiment to toxicity_n1024_fp: 100%|██████████| 1025/1025 [00:00<00:00, 4021.53it/s]\n",
      "adding sentiment to negative_n1024: 100%|██████████| 1025/1025 [00:00<00:00, 7118.54it/s]\n",
      "adding sentiment to negative_n64: 100%|██████████| 65/65 [00:00<00:00, 7107.14it/s]\n",
      "adding sentiment to positive_n1024: 100%|██████████| 1025/1025 [00:00<00:00, 7440.97it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_sentiment(exp_name):\n",
    "    for f in tqdm(os.listdir(f\"grid_results/{exp_name}\"), desc=f\"adding sentiment to {exp_name}\"):\n",
    "        if f == \"meta.json\":\n",
    "            continue\n",
    "\n",
    "        with open(f\"grid_results/{exp_name}/{f}\") as fp:\n",
    "            d = json.load(fp)\n",
    "\n",
    "        if \"sentiment\" in d:\n",
    "            continue # already computed\n",
    "\n",
    "        generations = [c[len(p):].replace(\"<|end_of_text|>\", \"\") for c, p in zip(d[\"completions\"], d[\"prompt_batch\"])]\n",
    "        results = sentiment_analysis(generations)\n",
    "        prob_positive = [r[\"score\"] if r[\"label\"] == \"POSITIVE\" else 1 - r[\"score\"] for r in results]\n",
    "        d[\"sentiment\"] = prob_positive\n",
    "\n",
    "        with open(f\"grid_results/{exp_name}/{f}\", \"w\") as fp:\n",
    "            json.dump(d, fp)\n",
    "\n",
    "for exp_name in os.listdir(\"grid_results\"):\n",
    "    add_sentiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m exp_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_results\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m         add_sentiment(exp_name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "while True:\n",
    "    time.sleep(60)\n",
    "    for exp_name in os.listdir(\"grid_results\"):\n",
    "        add_sentiment(exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompute perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05538b174f94d08910e97f44d436c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer\n",
      "Moving model to device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_grad_enabled(False)  # save memory\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_name = \"gpt2-xl\"\n",
    "device = \"cuda:0\"\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "  model.to(device)\n",
    "\n",
    "model.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function but do the forward pass for every input at once\n",
    "def compute_perplexities_batched(dataset, model, tokenizer, batch_size=8, **kwargs):\n",
    "    perplexities = []\n",
    "    sum_nllos = 0\n",
    "    n_tokens = 0\n",
    "    \n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Get input lengths for each sample in batch\n",
    "        input_lengths = []\n",
    "        combined_sentences = []\n",
    "        for _, sample in batch.iterrows():\n",
    "            if len(sample['generated']) == 0:\n",
    "                input_lengths.append(0)\n",
    "                continue\n",
    "            input_encodings = tokenizer(sample['input'], return_tensors='pt')\n",
    "            input_lengths.append(input_encodings['input_ids'].size(1))\n",
    "            combined_sentences.append(sample['input'] + sample['generated'])\n",
    "            \n",
    "        if not combined_sentences:\n",
    "            continue\n",
    "            \n",
    "        # Tokenize full batch\n",
    "        encodings = tokenizer(combined_sentences, return_tensors='pt', padding=True)\n",
    "        input_ids = encodings['input_ids'].to(model.device)\n",
    "        attention_mask = encodings['attention_mask'].to(model.device)\n",
    "        \n",
    "        import time\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            # logits = model(input_ids, labels=input_ids, attention_mask=attention_mask).logits # hf_model\n",
    "            logits = model(input_ids, attention_mask=attention_mask) # transformerlens model\n",
    "            logprobs = logits.log_softmax(dim=-1)\n",
    "        end = time.time()\n",
    "        print(f\"time: {end - start}\")\n",
    "        \n",
    "        loss_func = torch.nn.NLLLoss(ignore_index=-100, reduction='sum')\n",
    "        \n",
    "        # Calculate loss for each sample in batch\n",
    "        for j in range(len(combined_sentences)):\n",
    "            input_length = input_lengths[j]\n",
    "            if input_length == 0:\n",
    "                perplexities.append(None)\n",
    "                continue\n",
    "                \n",
    "            loss = loss_func(\n",
    "                logprobs[j, input_length:-1, :].contiguous(),\n",
    "                input_ids[j, input_length+1:].contiguous()\n",
    "            )\n",
    "            loss = loss.to(torch.float32).detach().cpu().numpy()\n",
    "            \n",
    "            n_tokens_here = (attention_mask[j, input_length+1:] == 1).sum().item()\n",
    "            \n",
    "            if n_tokens_here > 0:\n",
    "                perplexity = np.exp(loss / n_tokens_here)\n",
    "                sum_nllos += loss\n",
    "                n_tokens += n_tokens_here\n",
    "                if not np.isnan(perplexity):\n",
    "                    perplexities.append(perplexity)\n",
    "                else:\n",
    "                    perplexities.append(None)\n",
    "            else:\n",
    "                perplexities.append(None)\n",
    "                \n",
    "    return perplexities, sum_nllos, n_tokens\n",
    "\n",
    "\n",
    "def get_perplexity(dataset, model, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of the generated sentences.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset to be used for evaluation. Has columns \"input\" (for input text), \"generated\" (for generated text). \n",
    "        model (PreTrainedModel): The model to be evaluated.\n",
    "        tokenizer (Tokenizer): The tokenizer to be used for tokenizing the sentences.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "    \"\"\"\n",
    "    perplexities, sum_nllos, n_tokens = compute_perplexities_batched(dataset, model, model.tokenizer, **kwargs)\n",
    "\n",
    "    return {\n",
    "        \"average\": np.mean(perplexities),\n",
    "        \"median\": np.median(perplexities),\n",
    "        \"correct_perplexity\": np.exp(sum_nllos / n_tokens)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check batched fn. with different prompts (ugh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding sentiment to negative_n1024_fp:   0%|          | 0/1025 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.938963174819946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding sentiment to negative_n1024_fp:   0%|          | 0/1025 [00:11<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing 74.9001877784729 -> 127.94375292877415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def recompute_perplexity_and_sentiment(exp_name):\n",
    "    for f in tqdm(os.listdir(f\"grid_results/{exp_name}\"), desc=f\"adding sentiment to {exp_name}\"):\n",
    "        if f == \"meta.json\":\n",
    "            continue\n",
    "\n",
    "        with open(f\"grid_results/{exp_name}/{f}\") as fp:\n",
    "            d = json.load(fp)\n",
    "\n",
    "        # recompute perplexity\n",
    "        generated_only = [c[len(p):].replace(\"<|end_of_text|>\", \"\") for c, p in zip(d[\"completions\"], d[\"prompt_batch\"])]\n",
    "        prompt_only = [p.replace(\"<|end_of_text|>\", \"\") for p in d[\"prompt_batch\"]]\n",
    "\n",
    "        df = pd.DataFrame({\"input\": prompt_only, \"generated\": generated_only})\n",
    "        perplexities, sum_nllos, n_tokens = compute_perplexities_batched(df, model, model.tokenizer, batch_size=100)\n",
    "        d[\"correct_perplexity\"] = np.exp(sum_nllos / n_tokens)\n",
    "        d[\"perplexities_new\"] = perplexities\n",
    "\n",
    "        print(f\"existing {np.mean(d['perplexities'])} -> {d['correct_perplexity']}\")\n",
    "\n",
    "        break\n",
    "\n",
    "        # recompute sentiment\n",
    "        completions = [c.replace(\"<|end_of_text|>\", \"\") for c in d[\"completions\"]]\n",
    "        results = sentiment_analysis(completions)\n",
    "        prob_positive = [r[\"score\"] if r[\"label\"] == \"POSITIVE\" else 1 - r[\"score\"] for r in results]\n",
    "        d[\"sentiment_new\"] = prob_positive\n",
    "\n",
    "        with open(f\"grid_results/{exp_name}/{f}\", \"w\") as fp:\n",
    "            json.dump(d, fp)\n",
    "\n",
    "\n",
    "for exp_name in os.listdir(\"grid_results\"):\n",
    "    recompute_perplexity_and_sentiment(exp_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jean Dujardin gets Connery's mannerisms down p...</td>\n",
       "      <td>the silly grin he flashes at the camera, even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have seen so many bad reviews on Supervivien...</td>\n",
       "      <td>First, I deal with the question of what would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Looking backwards to that year 2002 when \"Furi...</td>\n",
       "      <td>. Comedians like Alina Pușcău, a veteran of Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE CELL (2000) Rating: 8/10&lt;br /&gt;&lt;br /&gt;The Ce...</td>\n",
       "      <td>Once in 2D and once in 3D which is included o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I loved this movie! It was all I could do not ...</td>\n",
       "      <td>by the great work done by the various actors ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>- After their sons are sentenced to life in pr...</td>\n",
       "      <td>calls from Brooks Halshaw (John Forsythe), th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love playing football and I thought this mov...</td>\n",
       "      <td>enjoyed watching this movie. However, there w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This Showtime cable film features a talented c...</td>\n",
       "      <td>experience to the sex addict who engages in r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This is a film i decided to go and see because...</td>\n",
       "      <td>around kids in the same way that most animati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movie is my all time favorite!!! You real...</td>\n",
       "      <td>is definitely my Top 3 Michael Jackson Music ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I simply can't get over how brilliant the pair...</td>\n",
       "      <td>; I get all the character development and stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Stuck in a hotel in Kuwait, I happily switched...</td>\n",
       "      <td>my throat.\\nI don't know a lot of the music, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Some of the background details of this story a...</td>\n",
       "      <td>elements of fiction and historical fact, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This is a pleasant film, even if the premise i...</td>\n",
       "      <td>this the old all the way and Troy do Yates is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>There must have been some interesting conversa...</td>\n",
       "      <td>in George Roy Hill's The Sting and Harold C. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I've been trying to find out about this series...</td>\n",
       "      <td>track it down since. It's definitely a little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I went to see this film out of curiosity, and ...</td>\n",
       "      <td>iev created to accompany it. Judy Garland play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Watching John Cassavetes film, Opening Night, ...</td>\n",
       "      <td>. He said:\\n“I don’t think there’s any such th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A teen-aged girl gets the horse of her dreams ...</td>\n",
       "      <td>In the course of her progress, various sorts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Although there is melodrama at the center or r...</td>\n",
       "      <td>/&gt;&lt;br/&gt;For a film about the deadly 'methamphet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I first saw this film over 25 years ago on Bri...</td>\n",
       "      <td>. Charming is too bland a word to describe the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ich will danke Herr Hacke für den Filme. Mein ...</td>\n",
       "      <td>I am so sorry for my English to be bad. I'm g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I saw this movie again as an assignment for my...</td>\n",
       "      <td>dont mind doing the commenting work on this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>This is, without a doubt, one of my favorite C...</td>\n",
       "      <td>and dialogue are quite interesting and the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Leland follows the story of Leland P. Fitzgera...</td>\n",
       "      <td>of his would-be lover and mentally unstable c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>VHS - I have watched this over and over and LO...</td>\n",
       "      <td>wasn’t over so quickly but loved it so much w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I have always loved The Muppets. Though most c...</td>\n",
       "      <td>were a weird mix of handmade puppets that loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Stan Laurel regarded PUTTING PANTS ON PHILIP a...</td>\n",
       "      <td>first feature film and THE GLORIOUS BOLSHIEK ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I'm not great at writing reviews, so I'll just...</td>\n",
       "      <td>of finding the golden eggs and the sneaking a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I loved Long Way Round and wasn't even aware o...</td>\n",
       "      <td>quickly loved it. A brilliant story of vivid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It couldn't.&lt;br /&gt;&lt;br /&gt;From the cutting dialo...</td>\n",
       "      <td>characters was a great job by the screenwrite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Van damme has done some great films over the y...</td>\n",
       "      <td>hotel where the love and action really get to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>I've been looking forward to seeing this film ...</td>\n",
       "      <td>Talented film. All the surprises I give it. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A River Runs Through It is based on the true s...</td>\n",
       "      <td>end father (Robert Redford), with his son's co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>This show was Fabulous. It was intricate and w...</td>\n",
       "      <td>husband you couldn't help but like and long f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>I heard tell that Madonna was briefly consider...</td>\n",
       "      <td>She lacks the necessary femininity and class ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Although I'm a girl, thankfully I have a sense...</td>\n",
       "      <td>of deja-vu, but every episode is pleasantly e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I remember watching this film a while ago and ...</td>\n",
       "      <td>a 5-star review on my Art Garfunkel page, Im ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>This movie is a piece of the time in which it ...</td>\n",
       "      <td>70s. They were very \"in touch\" with what was g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Pierce Brosnan has sipped his last Martini and...</td>\n",
       "      <td>— a successor to James Bond — with one foot i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>This is probably the only female Ninja movie e...</td>\n",
       "      <td>is a well done crowdfunded project and I defi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>I loved October Sky. The thing I loved most ha...</td>\n",
       "      <td>off a sense of the competition between the ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I first saw this one when it was first shown, ...</td>\n",
       "      <td>a fairly simple, gritty little ride, and part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Latter Days is a very, VERY independent movie....</td>\n",
       "      <td>is not very clean at times, the slices of dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I first saw this mini-series as a child and th...</td>\n",
       "      <td>, due to Disney's current policy of cataloging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Spirit and Chaos is an artistic biopic of Miya...</td>\n",
       "      <td>The movie is divided into two parts: “Early Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>most of the bad reviews on this website blame ...</td>\n",
       "      <td>...\\nBased on the short story \"A Truckload of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>This was a great anime. True the animation is ...</td>\n",
       "      <td>short 2 maybe 3 eps.\\nI have this on dvd and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>This movie deals with the European ERASMUS exc...</td>\n",
       "      <td>ch's film that I wouldn't like.\\nAxelle Laffon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>This movie is nothing short of a dark, gritty ...</td>\n",
       "      <td>South Africa. The problems with the story, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Movie about two Australian girls--Debbie (Nell...</td>\n",
       "      <td>fer dudes from Broken Bay in New South Wales.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>As a French, i found it very pleasant to be ab...</td>\n",
       "      <td>countries. But i was also disappointed in man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>This is a fine musical with a timeless score b...</td>\n",
       "      <td>Bostonians the inspiration to sing and dance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Okay. Here's the thing. I've read through the ...</td>\n",
       "      <td>most enjoyable work of kubrick they've seen. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Well I don't know much about anything, but I s...</td>\n",
       "      <td>. Okay, since that's all you need to know, I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>I'd even say some shades of Hitchcock...this i...</td>\n",
       "      <td>.\\nTaryn pg 10\\nI disagree with the sentiment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Well,I am a dancer so automatically I liked th...</td>\n",
       "      <td>be, replacing more dialogue. There didn't nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>I had mixed feelings for \"Les Valseuses\" (1974...</td>\n",
       "      <td>up loving it. \"Les Valseuses\" was remade as \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>If you are a fan of Zorro, Indiana Jones, or a...</td>\n",
       "      <td>Witney and John English and starring John Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>A woman who hates cats (Alice Krige) and her s...</td>\n",
       "      <td>cat. The townspeople form a posse to destroy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  \\\n",
       "0   Jean Dujardin gets Connery's mannerisms down p...   \n",
       "1   I have seen so many bad reviews on Supervivien...   \n",
       "2   Looking backwards to that year 2002 when \"Furi...   \n",
       "3   THE CELL (2000) Rating: 8/10<br /><br />The Ce...   \n",
       "4   I loved this movie! It was all I could do not ...   \n",
       "5   - After their sons are sentenced to life in pr...   \n",
       "6   I love playing football and I thought this mov...   \n",
       "7   This Showtime cable film features a talented c...   \n",
       "8   This is a film i decided to go and see because...   \n",
       "9   This movie is my all time favorite!!! You real...   \n",
       "10  I simply can't get over how brilliant the pair...   \n",
       "11  Stuck in a hotel in Kuwait, I happily switched...   \n",
       "12  Some of the background details of this story a...   \n",
       "13  This is a pleasant film, even if the premise i...   \n",
       "14  There must have been some interesting conversa...   \n",
       "15  I've been trying to find out about this series...   \n",
       "16  I went to see this film out of curiosity, and ...   \n",
       "17  Watching John Cassavetes film, Opening Night, ...   \n",
       "18  A teen-aged girl gets the horse of her dreams ...   \n",
       "19  Although there is melodrama at the center or r...   \n",
       "20  I first saw this film over 25 years ago on Bri...   \n",
       "21  Ich will danke Herr Hacke für den Filme. Mein ...   \n",
       "22  I saw this movie again as an assignment for my...   \n",
       "23  This is, without a doubt, one of my favorite C...   \n",
       "24  Leland follows the story of Leland P. Fitzgera...   \n",
       "25  VHS - I have watched this over and over and LO...   \n",
       "26  I have always loved The Muppets. Though most c...   \n",
       "27  Stan Laurel regarded PUTTING PANTS ON PHILIP a...   \n",
       "28  I'm not great at writing reviews, so I'll just...   \n",
       "29  I loved Long Way Round and wasn't even aware o...   \n",
       "30  It couldn't.<br /><br />From the cutting dialo...   \n",
       "31  Van damme has done some great films over the y...   \n",
       "32  I've been looking forward to seeing this film ...   \n",
       "33  A River Runs Through It is based on the true s...   \n",
       "34  This show was Fabulous. It was intricate and w...   \n",
       "35  I heard tell that Madonna was briefly consider...   \n",
       "36  Although I'm a girl, thankfully I have a sense...   \n",
       "37  I remember watching this film a while ago and ...   \n",
       "38  This movie is a piece of the time in which it ...   \n",
       "39  Pierce Brosnan has sipped his last Martini and...   \n",
       "40  This is probably the only female Ninja movie e...   \n",
       "41  I loved October Sky. The thing I loved most ha...   \n",
       "42  I first saw this one when it was first shown, ...   \n",
       "43  Latter Days is a very, VERY independent movie....   \n",
       "44  I first saw this mini-series as a child and th...   \n",
       "45  Spirit and Chaos is an artistic biopic of Miya...   \n",
       "46  most of the bad reviews on this website blame ...   \n",
       "47  This was a great anime. True the animation is ...   \n",
       "48  This movie deals with the European ERASMUS exc...   \n",
       "49  This movie is nothing short of a dark, gritty ...   \n",
       "50  Movie about two Australian girls--Debbie (Nell...   \n",
       "51  As a French, i found it very pleasant to be ab...   \n",
       "52  This is a fine musical with a timeless score b...   \n",
       "53  Okay. Here's the thing. I've read through the ...   \n",
       "54  Well I don't know much about anything, but I s...   \n",
       "55  I'd even say some shades of Hitchcock...this i...   \n",
       "56  Well,I am a dancer so automatically I liked th...   \n",
       "57  I had mixed feelings for \"Les Valseuses\" (1974...   \n",
       "58  If you are a fan of Zorro, Indiana Jones, or a...   \n",
       "59  A woman who hates cats (Alice Krige) and her s...   \n",
       "\n",
       "                                            generated  \n",
       "0    the silly grin he flashes at the camera, even...  \n",
       "1    First, I deal with the question of what would...  \n",
       "2   . Comedians like Alina Pușcău, a veteran of Ro...  \n",
       "3    Once in 2D and once in 3D which is included o...  \n",
       "4    by the great work done by the various actors ...  \n",
       "5    calls from Brooks Halshaw (John Forsythe), th...  \n",
       "6    enjoyed watching this movie. However, there w...  \n",
       "7    experience to the sex addict who engages in r...  \n",
       "8    around kids in the same way that most animati...  \n",
       "9    is definitely my Top 3 Michael Jackson Music ...  \n",
       "10  ; I get all the character development and stor...  \n",
       "11   my throat.\\nI don't know a lot of the music, ...  \n",
       "12   elements of fiction and historical fact, and ...  \n",
       "13   this the old all the way and Troy do Yates is...  \n",
       "14   in George Roy Hill's The Sting and Harold C. ...  \n",
       "15   track it down since. It's definitely a little...  \n",
       "16  iev created to accompany it. Judy Garland play...  \n",
       "17  . He said:\\n“I don’t think there’s any such th...  \n",
       "18   In the course of her progress, various sorts ...  \n",
       "19  /><br/>For a film about the deadly 'methamphet...  \n",
       "20  . Charming is too bland a word to describe the...  \n",
       "21   I am so sorry for my English to be bad. I'm g...  \n",
       "22   dont mind doing the commenting work on this m...  \n",
       "23   and dialogue are quite interesting and the se...  \n",
       "24   of his would-be lover and mentally unstable c...  \n",
       "25   wasn’t over so quickly but loved it so much w...  \n",
       "26   were a weird mix of handmade puppets that loo...  \n",
       "27   first feature film and THE GLORIOUS BOLSHIEK ...  \n",
       "28   of finding the golden eggs and the sneaking a...  \n",
       "29   quickly loved it. A brilliant story of vivid ...  \n",
       "30   characters was a great job by the screenwrite...  \n",
       "31   hotel where the love and action really get to...  \n",
       "32   Talented film. All the surprises I give it. A...  \n",
       "33  end father (Robert Redford), with his son's co...  \n",
       "34   husband you couldn't help but like and long f...  \n",
       "35   She lacks the necessary femininity and class ...  \n",
       "36   of deja-vu, but every episode is pleasantly e...  \n",
       "37   a 5-star review on my Art Garfunkel page, Im ...  \n",
       "38  70s. They were very \"in touch\" with what was g...  \n",
       "39   — a successor to James Bond — with one foot i...  \n",
       "40   is a well done crowdfunded project and I defi...  \n",
       "41   off a sense of the competition between the ki...  \n",
       "42   a fairly simple, gritty little ride, and part...  \n",
       "43   is not very clean at times, the slices of dia...  \n",
       "44  , due to Disney's current policy of cataloging...  \n",
       "45   The movie is divided into two parts: “Early Y...  \n",
       "46  ...\\nBased on the short story \"A Truckload of ...  \n",
       "47   short 2 maybe 3 eps.\\nI have this on dvd and ...  \n",
       "48  ch's film that I wouldn't like.\\nAxelle Laffon...  \n",
       "49   South Africa. The problems with the story, pl...  \n",
       "50  fer dudes from Broken Bay in New South Wales.\\...  \n",
       "51   countries. But i was also disappointed in man...  \n",
       "52   Bostonians the inspiration to sing and dance ...  \n",
       "53   most enjoyable work of kubrick they've seen. ...  \n",
       "54  . Okay, since that's all you need to know, I w...  \n",
       "55  .\\nTaryn pg 10\\nI disagree with the sentiment ...  \n",
       "56   be, replacing more dialogue. There didn't nee...  \n",
       "57   up loving it. \"Les Valseuses\" was remade as \"...  \n",
       "58   Witney and John English and starring John Car...  \n",
       "59   cat. The townspeople form a posse to destroy ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = \"positive_n64\"\n",
    "\n",
    "with open(f\"grid_results/{exp_name}/9_l3_c-1.json\") as fp:\n",
    "    d = json.load(fp)\n",
    "\n",
    "generated_only = [c[len(p):].replace(\"<|end_of_text|>\", \"\") for c, p in zip(d[\"completions\"], d[\"prompt_batch\"])]\n",
    "prompt_only = [p.replace(\"<|end_of_text|>\", \"\") for p in d[\"prompt_batch\"]]\n",
    "\n",
    "df = pd.DataFrame({\"input\": prompt_only, \"generated\": generated_only})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c959ad4c81a245028f941e790d75b879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "hf_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = hf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = hf_tokenizer(d[\"completions\"], return_tensors='pt')\n",
    "input_ids = encodings['input_ids'].to(hf_model.device)\n",
    "attention_mask = encodings['attention_mask'].to(hf_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "hf_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.818042039871216\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    logits = hf_model(input_ids, attention_mask=attention_mask)\n",
    "    end = time.time()\n",
    "    print(f\"time: {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
